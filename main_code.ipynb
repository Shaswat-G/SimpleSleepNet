{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRL Pre-Training with Linear Evalutaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import logging\n",
    "from typing import Dict, Optional\n",
    "\n",
    "eeg_data = {\n",
    "    'train': {label: [] for label in range(5)},\n",
    "    'validation': {label: [] for label in range(5)},\n",
    "    'test': {label: [] for label in range(5)},\n",
    "}\n",
    "\n",
    "dataset_path = \"./dset/Sleep-EDF-2018/npz/Fpz-Cz\"\n",
    "num_files_to_process = 200\n",
    "\n",
    "npz_files = sorted(glob.glob(os.path.join(dataset_path, '*.npz')))\n",
    "if not npz_files:\n",
    "    raise FileNotFoundError(f\"No .npz files found in {dataset_path}.\")\n",
    "if num_files_to_process is not None:\n",
    "    npz_files = npz_files[:num_files_to_process]\n",
    "\n",
    "# Extract subject indices from filenames\n",
    "subject_indices = []\n",
    "for npz_file in npz_files:\n",
    "    basename = os.path.basename(npz_file)\n",
    "    subject_idx = int(basename[3:5])\n",
    "    subject_indices.append(subject_idx)\n",
    "unique_subject_indices = list(set(subject_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_subject_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split subject indices into train, validation, test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_subjects, temp_subjects = train_test_split(unique_subject_indices, test_size=0.2)\n",
    "validation_subjects, test_subjects = train_test_split(temp_subjects, test_size=0.5)\n",
    "\n",
    "logger.info(f\"Subjects split into train ({len(train_subjects)}), validation ({len(validation_subjects)}), test ({len(test_subjects)}).\")\n",
    "\n",
    "# Process files and assign to corresponding sets\n",
    "for idx, npz_file in enumerate(npz_files, 1):\n",
    "    try:\n",
    "        basename = os.path.basename(npz_file)\n",
    "        subject_idx = int(basename[3:5])\n",
    "\n",
    "        # Determine the set for the current subject\n",
    "        if subject_idx in train_subjects:\n",
    "            set_name = 'train'\n",
    "        elif subject_idx in validation_subjects:\n",
    "            set_name = 'validation'\n",
    "        elif subject_idx in test_subjects:\n",
    "            set_name = 'test'\n",
    "        else:\n",
    "            logger.warning(f\"Subject index {subject_idx} not found in any set.\")\n",
    "            continue\n",
    "\n",
    "        with np.load(npz_file) as data:\n",
    "            eeg_epochs, labels = data['x'], data['y']\n",
    "            for label in range(5):\n",
    "                eeg_data[set_name][label].extend(eeg_epochs[labels == label])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {npz_file}: {e}\")\n",
    "    if idx % 10 == 0 or idx == len(npz_files):\n",
    "        logger.info(f\"Processed {idx}/{len(npz_files)} files.\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for set_name in eeg_data.keys():\n",
    "    for label in eeg_data[set_name].keys():\n",
    "        eeg_data[set_name][label] = np.array(eeg_data[set_name][label])\n",
    "\n",
    "eeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data = load_eeg_data(\"./dset/Sleep-EDF-2018/npz/Fpz-Cz\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0: 55834\n",
      "train 1: 16840\n",
      "train 2: 53972\n",
      "train 3: 10284\n",
      "train 4: 20483\n",
      "validation 0: 7091\n",
      "validation 1: 2410\n",
      "validation 2: 7889\n",
      "validation 3: 1039\n",
      "validation 4: 2563\n",
      "test 0: 6607\n",
      "test 1: 2141\n",
      "test 2: 6790\n",
      "test 3: 1677\n",
      "test 4: 2669\n"
     ]
    }
   ],
   "source": [
    "for key in eeg_data.keys():\n",
    "    for i in range(5):\n",
    "        print(f\"{key} {i}: {len(eeg_data[key][i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Set to avoid memory leak warning on Windows with MKL\n",
    "\n",
    "import umap\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE, trustworthiness\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, davies_bouldin_score, adjusted_rand_score,\n",
    "    adjusted_mutual_info_score\n",
    ")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.ndimage import shift\n",
    "from scipy.stats import entropy\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading EEG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 152 npz files in ./dset\\Sleep-EDF-2018\\npz\\Fpz-Cz.\n",
      "Loaded data from ./dset\\Sleep-EDF-2018\\npz\\Fpz-Cz\\SC4001E0.npz:\n",
      "EEG data shape (epochs, samples): (841, 3000)\n",
      "Labels shape: (841,)\n",
      "Percentage of each class :\n",
      "[[ 0.         22.35434007]\n",
      " [ 1.          6.89655172]\n",
      " [ 2.         29.72651605]\n",
      " [ 3.         26.15933413]\n",
      " [ 4.         14.86325803]]\n",
      "#EEG Epochs for Label 0 : 188\n",
      "#EEG Epochs for Label 1 : 58\n",
      "#EEG Epochs for Label 2 : 250\n",
      "#EEG Epochs for Label 3 : 220\n",
      "#EEG Epochs for Label 4 : 125\n"
     ]
    }
   ],
   "source": [
    "# config_path = 'config.json'\n",
    "# with open(config_path, 'r') as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# dset_cfg = config['dataset']\n",
    "# root_dir = dset_cfg['root_dir']\n",
    "# dset_name = dset_cfg['name']\n",
    "# eeg_channel = dset_cfg['eeg_channel']\n",
    "# num_splits = dset_cfg['num_splits']\n",
    "# seq_len = dset_cfg['seq_len']\n",
    "# target_idx = dset_cfg['target_idx']\n",
    "\n",
    "# print(\"Configuration Loaded:\")\n",
    "# print(json.dumps(dset_cfg, indent=4))\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "root_dir = \"./\"\n",
    "dset_name = \"Sleep-EDF-2018\"\n",
    "eeg_channel = \"Fpz-Cz\"\n",
    "dataset_path = os.path.join(root_dir, 'dset', dset_name, 'npz', eeg_channel)\n",
    "npz_files = sorted(glob.glob(os.path.join(dataset_path, '*.npz')))\n",
    "\n",
    "print(f\"Found {len(npz_files)} npz files in {dataset_path}.\")\n",
    "\n",
    "sample_file = npz_files[0]\n",
    "data = np.load(sample_file)\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "print(f\"Loaded data from {sample_file}:\")\n",
    "print(\"EEG data shape (epochs, samples):\", x.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# % of each class:\n",
    "print(\"Percentage of each class :\")\n",
    "print(np.asarray((unique, counts*100/np.sum(counts))).T)\n",
    "\n",
    "fs = 100\n",
    "epoch_length = 30\n",
    "samples_per_epoch = fs * epoch_length\n",
    "num_classes = 5\n",
    "epochs_per_class = 10\n",
    "\n",
    "eeg_data = {i: [] for i in range(5)}\n",
    "\n",
    "for file_idx, file in enumerate(npz_files, start=1):\n",
    "    with np.load(file) as data:\n",
    "        x = data['x']  # EEG epochs\n",
    "        y = data['y']  # Corresponding labels\n",
    "\n",
    "        for label in range(5):\n",
    "            epochs = x[y == label]\n",
    "            eeg_data[label].extend(epochs)\n",
    "    \n",
    "    if file_idx % 10 == 0 or file_idx == len(npz_files):\n",
    "        print(f\"Processed {file_idx}/{len(npz_files)} files.\")\n",
    "    break                                                                        # Remove this break\n",
    "\n",
    "for label in eeg_data:\n",
    "    eeg_data[label] = np.array(eeg_data[label])\n",
    "    print(f\"#EEG Epochs for Label {label} : {len(eeg_data[label])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRL Pre-Training of Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_augmentations import (\n",
    "    RandomAmplitudeScale,\n",
    "    RandomDCShift,\n",
    "    RandomTimeShift,\n",
    "    RandomZeroMasking,\n",
    "    RandomAdditiveGaussianNoise,\n",
    "    RandomBandStopFilter,\n",
    "    TimeWarping,\n",
    "    TimeReverse,\n",
    "    Permutation,\n",
    "    CutoutResize,\n",
    "    TailoredMixup,\n",
    "    AverageFilter,\n",
    "    SignFlip\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# class RandomAmplitudeScale:\n",
    "\n",
    "#     def __init__(self, range=(0.5, 2.0), p=0.5):\n",
    "#         self.range = range\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             scale = random.uniform(self.range[0], self.range[1])\n",
    "#             # print(f\"Scaled by {scale}\")\n",
    "#             return x * scale\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# class RandomDCShift:\n",
    "    \n",
    "#     def __init__(self, range=(-10.0, 10.0), p=0.5):\n",
    "#         self.range = range\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "\n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             shift_value = random.uniform(self.range[0], self.range[1])\n",
    "#             # print(f\"Shifted by {shift_value}\")\n",
    "#             return x + shift_value\n",
    "#         return x\n",
    "    \n",
    "# class RandomTimeShift:\n",
    "\n",
    "#     def __init__(self, range=(-300, 300), mode='constant', cval=0.0, p=0.5):\n",
    "#         self.range = range\n",
    "#         self.mode = mode\n",
    "#         self.cval = cval\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "\n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             t_shift = random.randint(self.range[0], self.range[1])\n",
    "#             x_shifted = shift(input=x, shift=t_shift, mode=self.mode, cval=self.cval)\n",
    "#             # print(f\"Time Shifted by : {t_shift}\")\n",
    "#             return x_shifted\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# class RandomZeroMasking:\n",
    "        \n",
    "#     def __init__(self, range=(0, 300), p=0.5):\n",
    "#         self.range = range\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "\n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             mask_len = random.randint(self.range[0], self.range[1])\n",
    "#             # print(f\"Mask Length : {mask_len}\")\n",
    "#             random_pos = random.randint(0, len(x) - mask_len)\n",
    "#             # print(f\"Position : {random_pos}\")\n",
    "#             mask = np.ones_like(x)\n",
    "#             mask[random_pos:random_pos + mask_len] = 0\n",
    "#             return x * mask\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# class RandomAdditiveGaussianNoise:\n",
    "        \n",
    "#     def __init__(self, range=(0.0, 0.2), p=0.5):\n",
    "#         self.range = range\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "\n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             sigma = random.uniform(self.range[0], self.range[1])\n",
    "#             noise = np.random.normal(0, sigma, x.shape)\n",
    "#             # print(f\"Gaussian Noise std = {sigma}\")\n",
    "#             return x + noise\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# class RandomBandStopFilter:\n",
    "\n",
    "#     def __init__(self, range=(0.5, 30.0), band_width=2.0, sampling_rate=100.0, p=0.5):\n",
    "#         self.range = range\n",
    "#         self.band_width = band_width\n",
    "#         self.sampling_rate = sampling_rate\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "\n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             low_freq = random.uniform(self.range[0], self.range[1] - self.band_width)\n",
    "#             center_freq = low_freq + self.band_width / 2.0\n",
    "#             # print(f\"Central Freq : {center_freq}\")\n",
    "#             b, a = signal.iirnotch(center_freq, Q=center_freq / self.band_width, fs=self.sampling_rate)\n",
    "#             x_filtered = signal.lfilter(b, a, x)\n",
    "#             return x_filtered\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# class TimeWarping:\n",
    "    \n",
    "#     def __init__(self, n_segments=4, scale_range=(0.5, 2.0), p=0.5):\n",
    "#         self.n_segments = n_segments\n",
    "#         self.scale_range = scale_range\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "        \n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             L = len(x) #3000\n",
    "\n",
    "#             segment_length = L // self.n_segments\n",
    "#             segments = []\n",
    "#             for i in range(self.n_segments):\n",
    "#                 start = i * segment_length\n",
    "#                 end = start + segment_length if i < self.n_segments -1 else L\n",
    "#                 Si = x[start:end]\n",
    "\n",
    "\n",
    "#                 omega = random.uniform(self.scale_range[0], self.scale_range[1])\n",
    "#                 new_length = int(len(Si) * omega)\n",
    "\n",
    "#                 if new_length < 1:\n",
    "#                     new_length = 1\n",
    "\n",
    "#                 Si_transformed = signal.resample(Si, new_length)\n",
    "#                 segments.append(Si_transformed)\n",
    "                \n",
    "#             x_aug = np.concatenate(segments)\n",
    "#             x_aug = signal.resample(x_aug, L)\n",
    "#             return x_aug\n",
    "        \n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "\n",
    "# class TimeReverse:\n",
    "    \n",
    "#     def __init__(self, p=0.5):\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "        \n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             return np.flip(x).copy()\n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "\n",
    "# class Permutation:\n",
    "    \n",
    "#     def __init__(self, n_segments=4, p=0.5):\n",
    "#         self.n_segments = n_segments\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "        \n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             L = len(x)\n",
    "\n",
    "#             segment_length = L // self.n_segments\n",
    "#             segments = []\n",
    "#             indices = []\n",
    "\n",
    "#             for i in range(self.n_segments):\n",
    "#                 start = i * segment_length\n",
    "#                 end = start + segment_length if i < self.n_segments -1 else L\n",
    "#                 Si = x[start:end]\n",
    "#                 segments.append(Si)\n",
    "#                 indices.append(i)\n",
    "\n",
    "#             random.shuffle(indices)\n",
    "#             shuffled_segments = [segments[i] for i in indices]\n",
    "#             x_aug = np.concatenate(shuffled_segments)\n",
    "#             return x_aug\n",
    "        \n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "\n",
    "# class CutoutResize:\n",
    "    \n",
    "#     def __init__(self, n_segments=4, p=0.5):\n",
    "#         self.n_segments = n_segments\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "        \n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             L = len(x)\n",
    "#             segment_length = L // self.n_segments\n",
    "#             segments = []\n",
    "#             for i in range(self.n_segments):\n",
    "#                 start = i * segment_length\n",
    "#                 end = start + segment_length if i < self.n_segments -1 else L\n",
    "#                 Si = x[start:end]\n",
    "#                 segments.append(Si)\n",
    "\n",
    "#             r = random.randint(0, self.n_segments - 1)\n",
    "#             del segments[r]\n",
    "\n",
    "#             x_aug = np.concatenate(segments)\n",
    "#             x_aug = signal.resample(x_aug, L)\n",
    "#             return x_aug\n",
    "        \n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "\n",
    "# class AverageFilter:\n",
    "    \n",
    "#     def __init__(self, k_range=(3, 10), p=0.5):\n",
    "#         self.k_range = k_range\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "        \n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             k = random.randint(self.k_range[0], self.k_range[1])\n",
    "#             kernel = np.ones(k) / k\n",
    "#             x_aug = np.convolve(x, kernel, mode='same')\n",
    "#             return x_aug\n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "\n",
    "# class SignFlip:\n",
    "    \n",
    "#     def __init__(self, p=0.5):\n",
    "#         self.p = p\n",
    "#         self.requires_x_random = False\n",
    "\n",
    "        \n",
    "#     def __call__(self, x, x_random=None):\n",
    "#         if torch.rand(1) < self.p:\n",
    "#             return (-x).copy()\n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "\n",
    "# class TailoredMixup:\n",
    "    \n",
    "#     def __init__(self, p=0.5, fs=100, beta=0.5):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         - p: probability of applying the mixup.\n",
    "#         - fs: sampling frequency of the signal.\n",
    "#         - beta: parameter for sampling mixup coefficients.\n",
    "#         \"\"\"\n",
    "#         self.p = p\n",
    "#         self.fs = fs\n",
    "#         self.beta = beta  # For simplicity, we'll use a fixed beta value here.\n",
    "#         self.requires_x_random = True  # Indicate that this augmentation requires x_random\n",
    "\n",
    "        \n",
    "#     def __call__(self, x_anchor, x_random=None):\n",
    "#         \"\"\"\n",
    "#         Apply the tailored mixup between x_anchor and x_random.\n",
    "        \n",
    "#         Parameters:\n",
    "#         - x_anchor: the anchor EEG signal (1D numpy array).\n",
    "#         - x_random: a randomly chosen EEG signal to mix with (1D numpy array).\n",
    "        \n",
    "#         Returns:\n",
    "#         - x_aug: the augmented EEG signal.\n",
    "#         \"\"\"\n",
    "#         if torch.rand(1) < self.p and x_random is not None:\n",
    "#             # Compute FFT of both signals\n",
    "#             X_anchor = np.fft.fft(x_anchor)\n",
    "#             X_random = np.fft.fft(x_random)\n",
    "            \n",
    "#             # Obtain magnitude and phase\n",
    "#             A_anchor = np.abs(X_anchor)\n",
    "#             P_anchor = np.angle(X_anchor)\n",
    "#             A_random = np.abs(X_random)\n",
    "#             P_random = np.angle(X_random)\n",
    "            \n",
    "#             # Mixup coefficients for magnitude and phase\n",
    "#             lambda_A = random.uniform(self.beta, 1.0)\n",
    "#             lambda_P = random.uniform(self.beta, 1.0)\n",
    "            \n",
    "#             # Mix magnitude\n",
    "#             A_mix = lambda_A * A_anchor + (1 - lambda_A) * A_random\n",
    "            \n",
    "#             # Compute phase difference\n",
    "#             delta_theta = P_anchor - P_random\n",
    "#             delta_theta = (delta_theta + np.pi) % (2 * np.pi) - np.pi  # Wrap between [-π, π]\n",
    "            \n",
    "#             # Adjust phase\n",
    "#             P_mix = P_anchor - delta_theta * (1 - lambda_P)\n",
    "            \n",
    "#             # Reconstruct the complex spectrum\n",
    "#             X_mix = A_mix * np.exp(1j * P_mix)\n",
    "            \n",
    "#             # Inverse FFT to get the augmented signal\n",
    "#             x_aug = np.fft.ifft(X_mix).real  # Take the real part\n",
    "            \n",
    "#             return x_aug\n",
    "#         else:\n",
    "#             return x_anchor.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sup_dataset import SupervisedEEGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crl_dataset import ContrastiveEEGDataset\n",
    "\n",
    "\n",
    "# class ContrastiveEEGDataset(Dataset):\n",
    "#     def __init__(self, eeg_data, augmentations=None, return_labels=False):\n",
    "#         \"\"\"\n",
    "#         eeg_data: Dictionary containing EEG signals per class.\n",
    "#         augmentations: A list of augmentation callables.\n",
    "#         return_labels: Whether to return labels in __getitem__.\n",
    "#         \"\"\"\n",
    "#         self.data = []\n",
    "#         self.labels = []\n",
    "#         for label, signals in eeg_data.items():\n",
    "#             self.data.extend(signals)\n",
    "#             self.labels.extend([label] * len(signals))\n",
    "#         self.data = np.array(self.data)\n",
    "#         self.labels = np.array(self.labels)\n",
    "#         self.augmentations = augmentations if augmentations is not None else []\n",
    "#         self.return_labels = return_labels\n",
    "        \n",
    "#         # Determine if any augmentation requires x_random\n",
    "#         self.requires_x_random = any(getattr(aug, 'requires_x_random', False) for aug in self.augmentations)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.data[idx]\n",
    "#         y = self.labels[idx]\n",
    "#         x_i = x.copy()\n",
    "#         x_j = x.copy()\n",
    "        \n",
    "#         # If any augmentation requires x_random, sample it\n",
    "#         if self.requires_x_random:\n",
    "#             idx_random = random.randint(0, len(self.data) - 1)\n",
    "#             x_random = self.data[idx_random]\n",
    "#         else:\n",
    "#             x_random = None\n",
    "        \n",
    "#         # Apply augmentations to x_i\n",
    "#         for aug in self.augmentations:\n",
    "#             if getattr(aug, 'requires_x_random', False):\n",
    "#                 x_i = aug(x_i, x_random.copy())\n",
    "#             else:\n",
    "#                 x_i = aug(x_i)\n",
    "        \n",
    "#         # Apply augmentations to x_j\n",
    "#         for aug in self.augmentations:\n",
    "#             if getattr(aug, 'requires_x_random', False):\n",
    "#                 x_j = aug(x_j, x_random.copy())\n",
    "#             else:\n",
    "#                 x_j = aug(x_j)\n",
    "        \n",
    "#         # Convert to torch tensors and add channel dimension\n",
    "#         x_i = torch.tensor(x_i, dtype=torch.float32).unsqueeze(0)\n",
    "#         x_j = torch.tensor(x_j, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "#         if self.return_labels:\n",
    "#             y = torch.tensor(y, dtype=torch.long)\n",
    "#             return x_i, x_j, y\n",
    "#         else:\n",
    "#             return x_i, x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleSLeepNet : Encoder Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SimpleSleepNet\n",
    "\n",
    "# class SimpleSleepNet(nn.Module):\n",
    "#     def __init__(self, latent_dim=128, dropout=0.5):\n",
    "#         super(SimpleSleepNet, self).__init__()\n",
    "        \n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "#         self.conv_path = nn.Sequential(\n",
    "#             # First Convolutional Block\n",
    "#             nn.Conv1d(in_channels=1, out_channels=32, kernel_size=64, stride=8, padding=32, bias=False),\n",
    "#             nn.BatchNorm1d(32),\n",
    "#             nn.ReLU(),\n",
    "#             self.dropout,\n",
    "            \n",
    "#             # Second Convolutional Block\n",
    "#             nn.Conv1d(in_channels=32, out_channels=64, kernel_size=32, stride=4, padding=16, bias=False),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.ReLU(),\n",
    "#             self.dropout,\n",
    "            \n",
    "#             # Third Convolutional Block\n",
    "#             nn.Conv1d(in_channels=64, out_channels=128, kernel_size=16, stride=2, padding=8, bias=False),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             self.dropout\n",
    "#         )\n",
    "        \n",
    "#         # Fully Connected Layer for Embedding\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(128, self.latent_dim),\n",
    "#             nn.BatchNorm1d(self.latent_dim),\n",
    "#             nn.ReLU(),\n",
    "#             self.dropout\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv_path(x)          # (Batch, 128, L')\n",
    "#         x = F.adaptive_avg_pool1d(x, 1)  # (Batch, 128, 1)\n",
    "#         x = x.view(x.size(0), -1)     # (Batch, 128)\n",
    "#         x = self.fc(x)                 # (Batch, latent_dim)\n",
    "#         x = F.normalize(x, p=2, dim=1)  # Normalize embeddings\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NT-Xent Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from loss import nt_xent_loss\n",
    "\n",
    "\n",
    "# def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "#     \"\"\"\n",
    "#     Computes the NT-Xent loss as introduced in SimCLR.\n",
    "\n",
    "#     Parameters:\n",
    "#     - z_i, z_j: Normalized embeddings of two augmented views (batch_size, embedding_dim).\n",
    "#     - temperature: Temperature scaling factor.\n",
    "\n",
    "#     Returns:\n",
    "#     - loss: The computed NT-Xent loss.\n",
    "#     \"\"\"\n",
    "#     batch_size = z_i.size(0)\n",
    "#     device = z_i.device\n",
    "\n",
    "#     # Normalize embeddings\n",
    "#     z_i = nn.functional.normalize(z_i, dim=1)\n",
    "#     z_j = nn.functional.normalize(z_j, dim=1)\n",
    "\n",
    "#     # Concatenate embeddings\n",
    "#     z = torch.cat([z_i, z_j], dim=0)  # Shape: (2*batch_size, embedding_dim)\n",
    "\n",
    "#     # Compute similarity matrix\n",
    "#     sim_matrix = torch.matmul(z, z.T) / temperature  # Shape: (2*batch_size, 2*batch_size)\n",
    "\n",
    "#     # Remove self-similarities\n",
    "#     mask = torch.eye(2 * batch_size, dtype=torch.bool).to(device)\n",
    "#     sim_matrix.masked_fill_(mask, -float('inf'))\n",
    "\n",
    "#     # Positive sample indices\n",
    "#     labels = torch.arange(batch_size).to(device)\n",
    "#     labels = torch.cat([labels + batch_size, labels], dim=0)\n",
    "\n",
    "#     # Compute loss\n",
    "#     loss = nn.CrossEntropyLoss()(sim_matrix, labels)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRL Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crl_pretraining import train_contrastive\n",
    "\n",
    "\n",
    "# def train_contrastive(model, dataloader, optimizer, num_epochs=5): #add temperature and device parameters to this class\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         for x_i, x_j in dataloader:\n",
    "#             x_i = x_i.to(device)\n",
    "#             x_j = x_j.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             z_i = model(x_i)\n",
    "#             z_j = model(x_j)\n",
    "            \n",
    "#             loss = nt_xent_loss(z_i, z_j, temperature=0.5)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         avg_loss = total_loss / len(dataloader)\n",
    "#         print(f\"Contrastive Training - Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LatentSpaceEvaluator:\n",
    "#     def __init__(\n",
    "#         self, model, dataloader, device='cpu', umap=True, pca=True, tsne=True,\n",
    "#         visualize=True, compute_metrics=True, n_clusters=5\n",
    "#     ):\n",
    "#         self.model = model.to(device)\n",
    "#         self.dataloader = dataloader\n",
    "#         self.device = device\n",
    "#         self.use_umap = umap\n",
    "#         self.use_pca = pca\n",
    "#         self.use_tsne = tsne\n",
    "#         self.visualize = visualize\n",
    "#         self.compute_metrics = compute_metrics\n",
    "#         self.n_clusters = n_clusters\n",
    "#         self.embeddings = None\n",
    "#         self.labels = None\n",
    "#         self.embeddings_scaled = None\n",
    "#         self.results = {}\n",
    "#         self.unique_labels = None\n",
    "\n",
    "#     def extract_embeddings(self):\n",
    "#         self.model.eval()\n",
    "#         embeddings = []\n",
    "#         labels = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in self.dataloader:\n",
    "#                 x_i, x_j, y = batch\n",
    "#                 x_i = x_i.to(self.device)\n",
    "#                 z_i = self.model(x_i)\n",
    "#                 embeddings.append(z_i.cpu().numpy())\n",
    "#                 labels.append(y.numpy())\n",
    "#         self.embeddings = np.concatenate(embeddings, axis=0)\n",
    "#         self.labels = np.concatenate(labels, axis=0)\n",
    "#         self.unique_labels = np.unique(self.labels)\n",
    "#         if len(self.unique_labels) != self.n_clusters:\n",
    "#             raise ValueError(\n",
    "#                 f\"Expected {self.n_clusters} unique labels, but found {len(self.unique_labels)}: {self.unique_labels}\"\n",
    "#             )\n",
    "#         scaler = StandardScaler()\n",
    "#         self.embeddings_scaled = scaler.fit_transform(self.embeddings)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def purity_score(y_true, y_pred):\n",
    "#         contingency_matrix = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))))\n",
    "#         for i, true_label in enumerate(np.unique(y_true)):\n",
    "#             for j, pred_label in enumerate(np.unique(y_pred)):\n",
    "#                 contingency_matrix[i, j] = np.sum(\n",
    "#                     (y_true == true_label) & (y_pred == pred_label)\n",
    "#                 )\n",
    "#         row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "#         purity = contingency_matrix[row_ind, col_ind].sum() / np.sum(contingency_matrix)\n",
    "#         return purity\n",
    "\n",
    "#     @staticmethod\n",
    "#     def compute_average_entropy(y_true, y_pred):\n",
    "#         cluster_labels = defaultdict(list)\n",
    "#         for label, cluster in zip(y_true, y_pred):\n",
    "#             cluster_labels[cluster].append(label)\n",
    "#         entropy_total = 0\n",
    "#         for labels_in_cluster in cluster_labels.values():\n",
    "#             label_counts = np.bincount(labels_in_cluster)\n",
    "#             probabilities = label_counts / len(labels_in_cluster)\n",
    "#             cluster_entropy = entropy(probabilities, base=2)\n",
    "#             entropy_total += cluster_entropy * len(labels_in_cluster)\n",
    "#         average_entropy = entropy_total / len(y_true)\n",
    "#         return average_entropy\n",
    "\n",
    "#     def apply_dimensionality_reduction(self):\n",
    "#         if self.use_tsne:\n",
    "#             tsne = TSNE(\n",
    "#                 n_components=2,\n",
    "#                 random_state=42,\n",
    "#                 perplexity=30,\n",
    "#                 n_iter=1000,\n",
    "#                 learning_rate='auto',\n",
    "#                 init='random',\n",
    "#                 verbose=1\n",
    "#             )\n",
    "#             embeddings_tsne = tsne.fit_transform(self.embeddings_scaled)\n",
    "#             self.results['t-SNE'] = {'embeddings': embeddings_tsne}\n",
    "\n",
    "#         if self.use_umap:\n",
    "#             with warnings.catch_warnings():\n",
    "#                 warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"umap\")\n",
    "#                 umap_reducer = umap.UMAP(\n",
    "#                     n_components=2,\n",
    "#                     random_state=42,\n",
    "#                     n_neighbors=15,\n",
    "#                     min_dist=0.1,\n",
    "#                     metric='euclidean',\n",
    "#                     n_jobs=1  # Explicitly set to suppress warning\n",
    "#                 )\n",
    "#                 embeddings_umap = umap_reducer.fit_transform(self.embeddings_scaled)\n",
    "#             self.results['UMAP'] = {'embeddings': embeddings_umap}\n",
    "\n",
    "#         if self.use_pca:\n",
    "#             pca = PCA(n_components=2, random_state=42)\n",
    "#             embeddings_pca = pca.fit_transform(self.embeddings_scaled)\n",
    "#             self.results['PCA'] = {'embeddings': embeddings_pca}\n",
    "\n",
    "#     def evaluate_metrics(self):\n",
    "#         for key, result in self.results.items():\n",
    "#             embeddings_2d = result['embeddings']\n",
    "#             metrics = {}\n",
    "#             # Clustering with KMeans\n",
    "#             kmeans = KMeans(\n",
    "#                 n_clusters=self.n_clusters,\n",
    "#                 random_state=42,\n",
    "#                 n_init='auto'  # Set explicitly to suppress FutureWarning\n",
    "#             )\n",
    "#             labels_pred = kmeans.fit_predict(embeddings_2d)\n",
    "#             metrics['Silhouette Score'] = silhouette_score(embeddings_2d, labels_pred)\n",
    "#             metrics['Davies-Bouldin Index'] = davies_bouldin_score(embeddings_2d, labels_pred)\n",
    "#             metrics['Adjusted Rand Index'] = adjusted_rand_score(self.labels, labels_pred)\n",
    "#             # Purity\n",
    "#             purity = self.purity_score(self.labels, labels_pred)\n",
    "#             metrics['Purity Score'] = purity\n",
    "#             # Average Entropy\n",
    "#             average_entropy = self.compute_average_entropy(self.labels, labels_pred)\n",
    "#             metrics['Average Entropy'] = average_entropy\n",
    "#             # Adjusted Mutual Information\n",
    "#             ami = adjusted_mutual_info_score(self.labels, labels_pred)\n",
    "#             metrics['Adjusted Mutual Information'] = ami\n",
    "#             # Trustworthiness\n",
    "#             trust = trustworthiness(self.embeddings_scaled, embeddings_2d, n_neighbors=5)\n",
    "#             metrics['Trustworthiness'] = trust\n",
    "#             # Continuity is not implemented\n",
    "#             metrics['Continuity'] = None\n",
    "#             self.results[key]['metrics'] = metrics\n",
    "\n",
    "#     def visualize_embeddings(self):\n",
    "#         # Use the new Matplotlib colormaps API to get the colormap\n",
    "#         cmap = matplotlib.colormaps.get_cmap('tab10')\n",
    "#         colors = [cmap(i / self.n_clusters) for i in range(self.n_clusters)]\n",
    "        \n",
    "#         for key, result in self.results.items():\n",
    "#             embeddings_2d = result['embeddings']\n",
    "#             plt.figure(figsize=(10, 8))\n",
    "#             for i, label in enumerate(self.unique_labels):\n",
    "#                 idxs = self.labels == label\n",
    "#                 plt.scatter(\n",
    "#                     embeddings_2d[idxs, 0],\n",
    "#                     embeddings_2d[idxs, 1],\n",
    "#                     label=f\"Class {label}\",\n",
    "#                     color=colors[i],\n",
    "#                     alpha=0.7,\n",
    "#                     edgecolors='k',\n",
    "#                     linewidth=0.5\n",
    "#                 )\n",
    "#             plt.legend(title=\"Classes\")\n",
    "#             plt.title(f\"{key} Visualization of Embeddings\")\n",
    "#             plt.xlabel(f\"{key} Dimension 1\")\n",
    "#             plt.ylabel(f\"{key} Dimension 2\")\n",
    "#             plt.grid(True)\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "#     def run(self):\n",
    "#         self.extract_embeddings()\n",
    "#         self.apply_dimensionality_reduction()\n",
    "#         if self.compute_metrics:\n",
    "#             self.evaluate_metrics()\n",
    "#             for key, result in self.results.items():\n",
    "#                 print(f\"\\n--- {key} Evaluation Metrics ---\")\n",
    "#                 metrics = result['metrics']\n",
    "#                 for metric, value in metrics.items():\n",
    "#                     if value is None:\n",
    "#                         print(f\"{metric}: N/A\")\n",
    "#                     else:\n",
    "#                         print(f\"{metric}: {value:.4f}\")\n",
    "#         if self.visualize:\n",
    "#             self.visualize_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import warnings\n",
    "# from collections import defaultdict\n",
    "# from pathlib import Path\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import (\n",
    "#     adjusted_rand_score,\n",
    "#     adjusted_mutual_info_score,\n",
    "#     davies_bouldin_score,\n",
    "#     silhouette_score\n",
    "# )\n",
    "# from sklearn.manifold import TSNE, trustworthiness\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.optimize import linear_sum_assignment\n",
    "# from scipy.stats import entropy\n",
    "# import umap\n",
    "\n",
    "# class LatentSpaceEvaluator:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model,\n",
    "#         dataloader,\n",
    "#         device='cpu',\n",
    "#         umap_enabled=True,\n",
    "#         pca_enabled=True,\n",
    "#         tsne_enabled=True,\n",
    "#         visualize=True,\n",
    "#         compute_metrics=True,\n",
    "#         n_clusters=5,\n",
    "#         output_image_dir='visualizations',\n",
    "#         output_metrics_dir='metrics',\n",
    "#         augmentation_strategy='default_strategy'\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initializes the LatentSpaceEvaluator.\n",
    "\n",
    "#         Parameters:\n",
    "#             model: The neural network model to extract embeddings from.\n",
    "#             dataloader: DataLoader providing the data batches.\n",
    "#             device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "#             umap_enabled (bool): Whether to use UMAP for dimensionality reduction.\n",
    "#             pca_enabled (bool): Whether to use PCA for dimensionality reduction.\n",
    "#             tsne_enabled (bool): Whether to use t-SNE for dimensionality reduction.\n",
    "#             visualize (bool): Whether to visualize embeddings.\n",
    "#             compute_metrics (bool): Whether to compute evaluation metrics.\n",
    "#             n_clusters (int): Number of expected clusters/classes.\n",
    "#             output_image_dir (str): Directory path to save visualization images.\n",
    "#             output_metrics_dir (str): Directory path to save metrics CSV files.\n",
    "#             augmentation_strategy (str): Placeholder for data-augmentation strategy.\n",
    "#         \"\"\"\n",
    "#         self.model = model.to(device)\n",
    "#         self.dataloader = dataloader\n",
    "#         self.device = device\n",
    "#         self.use_umap = umap_enabled\n",
    "#         self.use_pca = pca_enabled\n",
    "#         self.use_tsne = tsne_enabled\n",
    "#         self.visualize = visualize\n",
    "#         self.compute_metrics = compute_metrics\n",
    "#         self.n_clusters = n_clusters\n",
    "#         self.embeddings = None\n",
    "#         self.labels = None\n",
    "#         self.embeddings_scaled = None\n",
    "#         self.results = {}\n",
    "#         self.unique_labels = None\n",
    "#         self.augmentation_strategy = augmentation_strategy\n",
    "\n",
    "#         # Set up output directories\n",
    "#         self.output_image_dir = Path(output_image_dir)\n",
    "#         self.output_metrics_dir = Path(output_metrics_dir)\n",
    "#         self.output_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "#         self.output_metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         # Set Seaborn style for better aesthetics\n",
    "#         sns.set_theme(style='whitegrid', context='talk')\n",
    "#         matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "#     def extract_embeddings(self):\n",
    "#         \"\"\"\n",
    "#         Extracts embeddings from the model using the provided dataloader.\n",
    "#         \"\"\"\n",
    "#         self.model.eval()\n",
    "#         embeddings = []\n",
    "#         labels = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in self.dataloader:\n",
    "#                 # Assuming batch contains (x_i, x_j, y)\n",
    "#                 x_i, _, y = batch\n",
    "#                 x_i = x_i.to(self.device)\n",
    "#                 z_i = self.model(x_i)\n",
    "#                 embeddings.append(z_i.cpu().numpy())\n",
    "#                 labels.append(y.numpy())\n",
    "#         self.embeddings = np.concatenate(embeddings, axis=0)\n",
    "#         self.labels = np.concatenate(labels, axis=0)\n",
    "#         self.unique_labels = np.unique(self.labels)\n",
    "#         if len(self.unique_labels) != self.n_clusters:\n",
    "#             raise ValueError(\n",
    "#                 f\"Expected {self.n_clusters} unique labels, but found {len(self.unique_labels)}: {self.unique_labels}\"\n",
    "#             )\n",
    "#         scaler = StandardScaler()\n",
    "#         self.embeddings_scaled = scaler.fit_transform(self.embeddings)\n",
    "#         print(\"Embeddings extracted and scaled.\")\n",
    "\n",
    "#     @staticmethod\n",
    "#     def purity_score(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         Calculates the purity score for the clustering.\n",
    "\n",
    "#         Parameters:\n",
    "#             y_true (array-like): True labels.\n",
    "#             y_pred (array-like): Predicted cluster labels.\n",
    "\n",
    "#         Returns:\n",
    "#             float: Purity score.\n",
    "#         \"\"\"\n",
    "#         contingency_matrix = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))))\n",
    "#         for i, true_label in enumerate(np.unique(y_true)):\n",
    "#             for j, pred_label in enumerate(np.unique(y_pred)):\n",
    "#                 contingency_matrix[i, j] = np.sum(\n",
    "#                     (y_true == true_label) & (y_pred == pred_label)\n",
    "#                 )\n",
    "#         row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "#         purity = contingency_matrix[row_ind, col_ind].sum() / np.sum(contingency_matrix)\n",
    "#         return purity\n",
    "\n",
    "#     @staticmethod\n",
    "#     def compute_average_entropy(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         Computes the average entropy for the clustering.\n",
    "\n",
    "#         Parameters:\n",
    "#             y_true (array-like): True labels.\n",
    "#             y_pred (array-like): Predicted cluster labels.\n",
    "\n",
    "#         Returns:\n",
    "#             float: Average entropy.\n",
    "#         \"\"\"\n",
    "#         cluster_labels = defaultdict(list)\n",
    "#         for label, cluster in zip(y_true, y_pred):\n",
    "#             cluster_labels[cluster].append(label)\n",
    "#         entropy_total = 0\n",
    "#         for labels_in_cluster in cluster_labels.values():\n",
    "#             label_counts = np.bincount(labels_in_cluster)\n",
    "#             probabilities = label_counts / len(labels_in_cluster)\n",
    "#             # Filter out zero probabilities to avoid log2(0)\n",
    "#             probabilities = probabilities[probabilities > 0]\n",
    "#             cluster_entropy = entropy(probabilities, base=2)\n",
    "#             entropy_total += cluster_entropy * len(labels_in_cluster)\n",
    "#         average_entropy = entropy_total / len(y_true)\n",
    "#         return average_entropy\n",
    "\n",
    "#     def apply_dimensionality_reduction(self):\n",
    "#         \"\"\"\n",
    "#         Applies the selected dimensionality reduction techniques to the embeddings.\n",
    "#         \"\"\"\n",
    "#         if self.use_tsne:\n",
    "#             print(\"Applying t-SNE...\")\n",
    "#             tsne = TSNE(\n",
    "#                 n_components=2,\n",
    "#                 random_state=42,\n",
    "#                 perplexity=30,\n",
    "#                 n_iter=1000,\n",
    "#                 learning_rate='auto',\n",
    "#                 init='random',\n",
    "#                 verbose=0  # Set to 1 for detailed output\n",
    "#             )\n",
    "#             embeddings_tsne = tsne.fit_transform(self.embeddings_scaled)\n",
    "#             self.results['t-SNE'] = {'embeddings': embeddings_tsne}\n",
    "#             print(\"t-SNE completed.\")\n",
    "\n",
    "#         if self.use_umap:\n",
    "#             print(\"Applying UMAP...\")\n",
    "#             with warnings.catch_warnings():\n",
    "#                 warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"umap\")\n",
    "#                 umap_reducer = umap.UMAP(\n",
    "#                     n_components=2,\n",
    "#                     random_state=42,\n",
    "#                     n_neighbors=15,\n",
    "#                     min_dist=0.1,\n",
    "#                     metric='euclidean',\n",
    "#                     n_jobs=-1  # Use all available cores\n",
    "#                 )\n",
    "#                 embeddings_umap = umap_reducer.fit_transform(self.embeddings_scaled)\n",
    "#             self.results['UMAP'] = {'embeddings': embeddings_umap}\n",
    "#             print(\"UMAP completed.\")\n",
    "\n",
    "#         if self.use_pca:\n",
    "#             print(\"Applying PCA...\")\n",
    "#             pca = PCA(n_components=2, random_state=42)\n",
    "#             embeddings_pca = pca.fit_transform(self.embeddings_scaled)\n",
    "#             self.results['PCA'] = {'embeddings': embeddings_pca}\n",
    "#             print(\"PCA completed.\")\n",
    "\n",
    "#     def evaluate_metrics(self):\n",
    "#         \"\"\"\n",
    "#         Evaluates clustering metrics for each dimensionality reduction result.\n",
    "#         \"\"\"\n",
    "#         print(\"Evaluating metrics...\")\n",
    "#         for key, result in self.results.items():\n",
    "#             embeddings_2d = result['embeddings']\n",
    "#             metrics = {}\n",
    "#             # Clustering with KMeans\n",
    "#             kmeans = KMeans(\n",
    "#                 n_clusters=self.n_clusters,\n",
    "#                 random_state=42,\n",
    "#                 n_init='auto'  # Set explicitly to suppress FutureWarning\n",
    "#             )\n",
    "#             labels_pred = kmeans.fit_predict(embeddings_2d)\n",
    "#             metrics['Silhouette Score'] = silhouette_score(embeddings_2d, labels_pred)\n",
    "#             metrics['Davies-Bouldin Index'] = davies_bouldin_score(embeddings_2d, labels_pred)\n",
    "#             metrics['Adjusted Rand Index'] = adjusted_rand_score(self.labels, labels_pred)\n",
    "#             # Purity\n",
    "#             purity = self.purity_score(self.labels, labels_pred)\n",
    "#             metrics['Purity Score'] = purity\n",
    "#             # Average Entropy\n",
    "#             average_entropy = self.compute_average_entropy(self.labels, labels_pred)\n",
    "#             metrics['Average Entropy'] = average_entropy\n",
    "#             # Adjusted Mutual Information\n",
    "#             ami = adjusted_mutual_info_score(self.labels, labels_pred)\n",
    "#             metrics['Adjusted Mutual Information'] = ami\n",
    "#             # Trustworthiness\n",
    "#             trust = trustworthiness(self.embeddings_scaled, embeddings_2d, n_neighbors=5)\n",
    "#             metrics['Trustworthiness'] = trust\n",
    "#             # Continuity is not implemented\n",
    "#             metrics['Continuity'] = None\n",
    "#             self.results[key]['metrics'] = metrics\n",
    "\n",
    "#             # Save metrics to CSV\n",
    "#             metrics_df = pd.DataFrame([metrics])\n",
    "#             metrics_filename = f\"{key.lower()}_metrics_{self.augmentation_strategy}.csv\"\n",
    "#             metrics_path = self.output_metrics_dir / metrics_filename\n",
    "#             metrics_df.to_csv(metrics_path, index=False)\n",
    "#             print(f\"Metrics for {key} saved to {metrics_path}.\")\n",
    "\n",
    "#     def visualize_embeddings(self):\n",
    "#         \"\"\"\n",
    "#         Visualizes the embeddings and saves the plots as image files.\n",
    "#         \"\"\"\n",
    "#         print(\"Visualizing embeddings...\")\n",
    "#         # Define color palette\n",
    "#         palette = sns.color_palette(\"tab10\", n_colors=self.n_clusters)\n",
    "\n",
    "#         for key, result in self.results.items():\n",
    "#             embeddings_2d = result['embeddings']\n",
    "#             plt.figure(figsize=(12, 10))\n",
    "#             sns.scatterplot(\n",
    "#                 x=embeddings_2d[:, 0],\n",
    "#                 y=embeddings_2d[:, 1],\n",
    "#                 hue=self.labels,\n",
    "#                 palette=palette,\n",
    "#                 legend='full',\n",
    "#                 alpha=0.7,\n",
    "#                 edgecolor='k',\n",
    "#                 linewidth=0.5\n",
    "#             )\n",
    "#             plt.title(f\"{key} Visualization of Embeddings\", fontsize=16)\n",
    "#             plt.xlabel(f\"{key} Dimension 1\", fontsize=14)\n",
    "#             plt.ylabel(f\"{key} Dimension 2\", fontsize=14)\n",
    "#             plt.legend(title=\"Classes\", fontsize=12, title_fontsize=13, loc='best')\n",
    "#             plt.tight_layout()\n",
    "\n",
    "#             # Save the plot\n",
    "#             image_filename = f\"{key.lower()}_embeddings_{self.augmentation_strategy}.png\"\n",
    "#             image_path = self.output_image_dir / image_filename\n",
    "#             plt.savefig(image_path, dpi=300)\n",
    "#             plt.close()\n",
    "#             print(f\"Visualization for {key} saved to {image_path}.\")\n",
    "\n",
    "#     def run(self):\n",
    "#         \"\"\"\n",
    "#         Executes the full evaluation pipeline: extraction, dimensionality reduction,\n",
    "#         metric evaluation, and visualization.\n",
    "#         \"\"\"\n",
    "#         print(\"Starting Latent Space Evaluation...\")\n",
    "#         self.extract_embeddings()\n",
    "#         self.apply_dimensionality_reduction()\n",
    "#         if self.compute_metrics:\n",
    "#             self.evaluate_metrics()\n",
    "#             print(\"Evaluation metrics computed and saved.\")\n",
    "#         if self.visualize:\n",
    "#             self.visualize_embeddings()\n",
    "#             print(\"Embeddings visualized and saved.\")\n",
    "#         print(\"Latent Space Evaluation Completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train Encoder with CRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Contrastive Training - Epoch [1/10], Loss: 5.3588\n",
      "Contrastive Training - Epoch [2/10], Loss: 5.3384\n",
      "Contrastive Training - Epoch [3/10], Loss: 5.3210\n",
      "Contrastive Training - Epoch [4/10], Loss: 5.2958\n",
      "Contrastive Training - Epoch [5/10], Loss: 5.2447\n",
      "Contrastive Training - Epoch [6/10], Loss: 5.2447\n",
      "Contrastive Training - Epoch [7/10], Loss: 5.1947\n",
      "Contrastive Training - Epoch [8/10], Loss: 5.1642\n",
      "Contrastive Training - Epoch [9/10], Loss: 5.1218\n",
      "Contrastive Training - Epoch [10/10], Loss: 5.0927\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "augmentations = [RandomAmplitudeScale(p=1.0),TailoredMixup(p=1.0),RandomAdditiveGaussianNoise(p=1.0)]\n",
    "\n",
    "contrastive_dataset = ContrastiveEEGDataset(eeg_data, augmentations=augmentations)\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=128, shuffle=True)  #batchsize = 64\n",
    "\n",
    "model = SimpleSleepNet(latent_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_contrastive(model, contrastive_loader, optimizer, device=device, num_epochs=10, temperature = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Latent Space Evaluation...\n",
      "Embeddings extracted and scaled.\n",
      "Applying t-SNE...\n",
      "t-SNE completed.\n",
      "Applying UMAP...\n",
      "UMAP completed.\n",
      "Applying PCA...\n",
      "PCA completed.\n",
      "Evaluating metrics...\n",
      "Metrics for t-SNE saved to latent_space_metrics\\t-sne_metrics_1.csv.\n",
      "Metrics for UMAP saved to latent_space_metrics\\umap_metrics_1.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for PCA saved to latent_space_metrics\\pca_metrics_1.csv.\n",
      "Evaluation metrics computed and saved.\n",
      "Visualizing embeddings...\n",
      "Visualization for t-SNE saved to latent_space_viz\\t-sne_embeddings_1.png.\n",
      "Visualization for UMAP saved to latent_space_viz\\umap_embeddings_1.png.\n",
      "Visualization for PCA saved to latent_space_viz\\pca_embeddings_1.png.\n",
      "Embeddings visualized and saved.\n",
      "Latent Space Evaluation Completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset with return_labels=True\n",
    "visualization_dataset = ContrastiveEEGDataset(\n",
    "    eeg_data= eeg_data,\n",
    "    augmentations=[],\n",
    "    return_labels=True\n",
    ")\n",
    "\n",
    "visualization_loader = DataLoader(visualization_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# evaluator = LatentSpaceEvaluator(\n",
    "#     model=model,\n",
    "#     dataloader=visualization_loader,\n",
    "#     device='cuda',       # or 'cpu'\n",
    "#     umap=True,\n",
    "#     pca=True,\n",
    "#     tsne=True,\n",
    "#     visualize=True,\n",
    "#     compute_metrics=True,\n",
    "#     n_clusters=5         # Number of expected clusters/classes\n",
    "# )\n",
    "\n",
    "# evaluator.run()\n",
    "\n",
    "# Assuming you have already defined `model` and `visualization_loader`\n",
    "\n",
    "\n",
    "from latent_space_evaluator import LatentSpaceEvaluator\n",
    "\n",
    "evaluator = LatentSpaceEvaluator(\n",
    "    model=model,\n",
    "    dataloader=visualization_loader,\n",
    "    device='cuda',                   # or 'cpu'\n",
    "    umap_enabled=True,\n",
    "    pca_enabled=True,\n",
    "    tsne_enabled=True,\n",
    "    visualize=True,\n",
    "    compute_metrics=True,\n",
    "    n_clusters=5,                   # Number of expected clusters/classes\n",
    "    output_image_dir='latent_space_viz',    # Specify your desired image output directory\n",
    "    output_metrics_dir='latent_space_metrics', # Specify your desired metrics output directory\n",
    "    augmentation_strategy='1' # Replace with your augmentation strategy identifier\n",
    ")\n",
    "\n",
    "evaluator.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SleepStageClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SleepStageClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim=128, num_classes=5):\n",
    "#         super(SleepStageClassifier, self).__init__()\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(128, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class SleepStageClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim=128, num_classes=5, dropout_probs=(0.5, 0.5)):\n",
    "#         \"\"\"\n",
    "#         Sleep stage classifier with configurable dropout rates.\n",
    "\n",
    "#         Parameters:\n",
    "#         - input_dim (int): Dimensionality of the input features.\n",
    "#         - num_classes (int): Number of classes to predict.\n",
    "#         - dropout_probs (tuple): Dropout probabilities for the layers.\n",
    "#         \"\"\"\n",
    "#         super(SleepStageClassifier, self).__init__()\n",
    "        \n",
    "#         # Ensure dropout_probs is a tuple of the correct size\n",
    "#         if not isinstance(dropout_probs, (tuple, list)) or len(dropout_probs) != 2:\n",
    "#             raise ValueError(\"dropout_probs must be a tuple or list of length 2.\")\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=dropout_probs[0]),  # First dropout rate\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=dropout_probs[1]),  # Second dropout rate\n",
    "#             nn.Linear(128, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the classifier.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x (torch.Tensor): Input tensor of shape (Batch, input_dim).\n",
    "\n",
    "#         Returns:\n",
    "#         - torch.Tensor: Output logits of shape (Batch, num_classes).\n",
    "#         \"\"\"\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SimpleSleepNet, SleepStageClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedEEGDataset(Dataset):\n",
    "#     def __init__(self, eeg_data):\n",
    "\n",
    "#         self.data = []\n",
    "#         self.labels = []\n",
    "\n",
    "#         for label, signals in eeg_data.items():\n",
    "#             self.data.extend(signals)\n",
    "#             self.labels.extend([label] * len(signals))\n",
    "\n",
    "#         self.data = np.array(self.data)\n",
    "#         self.labels = np.array(self.labels)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.data[idx]\n",
    "#         y = self.labels[idx]\n",
    "#         x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "#         y = torch.tensor(y, dtype=torch.long)\n",
    "#         return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine all data and labels\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for label, signals in eeg_data.items():\n",
    "    all_data.extend(signals)\n",
    "    all_labels.extend([label] * len(signals))\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# First split into train+val and test sets (80% train+val, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    all_data, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Then split train_val into train and val sets (75% train, 25% val of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42\n",
    ")\n",
    "# Now we have 60% train, 20% val, 20% test\n",
    "\n",
    "# Create dictionaries to store data by label\n",
    "num_classes = len(np.unique(all_labels))\n",
    "train_data = {label: [] for label in range(num_classes)}\n",
    "val_data = {label: [] for label in range(num_classes)}\n",
    "test_data = {label: [] for label in range(num_classes)}\n",
    "\n",
    "# Fill the dictionaries\n",
    "for x, y in zip(X_train, y_train):\n",
    "    train_data[y].append(x)\n",
    "for x, y in zip(X_val, y_val):\n",
    "    val_data[y].append(x)\n",
    "for x, y in zip(X_test, y_test):\n",
    "    test_data[y].append(x)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for label in train_data:\n",
    "    train_data[label] = np.array(train_data[label])\n",
    "    val_data[label] = np.array(val_data[label])\n",
    "    test_data[label] = np.array(test_data[label])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SupervisedEEGDataset(train_data)\n",
    "val_dataset = SupervisedEEGDataset(val_data)\n",
    "test_dataset = SupervisedEEGDataset(test_data)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128   #64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_classifier(encoder, classifier, data_loader, criterion):\n",
    "#     encoder.eval()\n",
    "#     classifier.eval()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for x, y in data_loader:\n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "            \n",
    "#             z = encoder(x)\n",
    "#             outputs = classifier(z)\n",
    "#             loss = criterion(outputs, y)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct += (preds == y).sum().item()\n",
    "#             total += y.size(0)\n",
    "    \n",
    "#     avg_loss = total_loss / len(data_loader)\n",
    "#     accuracy = correct / total\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# def train_classifier(encoder, classifier, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "#     encoder.eval()  # Set encoder to evaluation mode\n",
    "    \n",
    "#     best_val_loss = float('inf')\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         classifier.train()  # Set classifier to training mode\n",
    "#         total_loss = 0.0\n",
    "        \n",
    "#         for x, y in train_loader:\n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 z = encoder(x)  # Get embeddings from frozen encoder\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = classifier(z)\n",
    "#             loss = criterion(outputs, y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "#         # Evaluate on validation set\n",
    "#         val_loss, val_accuracy = evaluate_classifier(encoder, classifier, val_loader, criterion)\n",
    "        \n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "#               f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "#               f\"Val Loss: {val_loss:.4f}, \"\n",
    "#               f\"Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "#         # Save the best model based on validation loss\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             torch.save(classifier.state_dict(), 'best_classifier.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "\n",
    "# def evaluate_classifier(encoder, classifier, data_loader, criterion, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Evaluates the classifier on a given dataset.\n",
    "\n",
    "#     Parameters:\n",
    "#     - encoder (nn.Module): Frozen encoder to generate embeddings.\n",
    "#     - classifier (nn.Module): Classifier to evaluate.\n",
    "#     - data_loader (DataLoader): DataLoader providing the evaluation dataset.\n",
    "#     - criterion (nn.Module): Loss function.\n",
    "#     - device (str): Device to run the evaluation on ('cuda' or 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#     - avg_loss (float): Average loss over the dataset.\n",
    "#     - accuracy (float): Classification accuracy.\n",
    "#     \"\"\"\n",
    "#     encoder.eval()\n",
    "#     classifier.eval()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for x, y in data_loader:\n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "            \n",
    "#             z = encoder(x)\n",
    "#             outputs = classifier(z)\n",
    "#             loss = criterion(outputs, y)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct += (preds == y).sum().item()\n",
    "#             total += y.size(0)\n",
    "    \n",
    "#     avg_loss = total_loss / len(data_loader)\n",
    "#     accuracy = correct / total\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# def train_classifier(\n",
    "#     encoder,\n",
    "#     classifier,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     criterion,\n",
    "#     optimizer,\n",
    "#     num_epochs=50,\n",
    "#     device='cuda',\n",
    "#     save_path='best_classifier.pth'\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Trains the classifier while keeping the encoder frozen.\n",
    "\n",
    "#     Parameters:\n",
    "#     - encoder (nn.Module): Frozen encoder to generate embeddings.\n",
    "#     - classifier (nn.Module): Classifier to train.\n",
    "#     - train_loader (DataLoader): DataLoader for the training set.\n",
    "#     - val_loader (DataLoader): DataLoader for the validation set.\n",
    "#     - criterion (nn.Module): Loss function.\n",
    "#     - optimizer (Optimizer): Optimizer for the classifier.\n",
    "#     - num_epochs (int): Number of training epochs.\n",
    "#     - device (str): Device to run the training on ('cuda' or 'cpu').\n",
    "#     - save_path (str): Path to save the best classifier model.\n",
    "\n",
    "#     Returns:\n",
    "#     - best_val_loss (float): Best validation loss achieved during training.\n",
    "#     \"\"\"\n",
    "#     encoder.eval()  # Set encoder to evaluation mode\n",
    "#     classifier.to(device)\n",
    "#     best_val_loss = float('inf')\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         classifier.train()  # Set classifier to training mode\n",
    "#         total_loss = 0.0\n",
    "        \n",
    "#         for x, y in train_loader:\n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 z = encoder(x)  # Get embeddings from frozen encoder\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = classifier(z)\n",
    "#             loss = criterion(outputs, y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "#         # Evaluate on validation set\n",
    "#         val_loss, val_accuracy = evaluate_classifier(encoder, classifier, val_loader, criterion, device)\n",
    "        \n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "#               f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "#               f\"Val Loss: {val_loss:.4f}, \"\n",
    "#               f\"Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "#         # Save the best model based on validation loss\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             torch.save(classifier.state_dict(), save_path)\n",
    "#             print(f\"Saved best model to {save_path}\")\n",
    "    \n",
    "#     return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_classifier(encoder, classifier, data_loader, criterion):\n",
    "#     encoder.eval()\n",
    "#     classifier.eval()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for x, y in data_loader:\n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "            \n",
    "#             z = encoder(x)\n",
    "#             outputs = classifier(z)\n",
    "#             loss = criterion(outputs, y)\n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct += (preds == y).sum().item()\n",
    "#             total += y.size(0)\n",
    "    \n",
    "#     avg_loss = total_loss / len(data_loader)\n",
    "#     accuracy = correct / total\n",
    "#     return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# def get_predictions(encoder, classifier, data_loader, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Generates predictions for a dataset using the encoder and classifier.\n",
    "\n",
    "#     Parameters:\n",
    "#     - encoder (nn.Module): Pretrained encoder model.\n",
    "#     - classifier (nn.Module): Classifier model.\n",
    "#     - data_loader (DataLoader): DataLoader providing the dataset.\n",
    "#     - device (str): Device to run the models on ('cuda' or 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#     - all_preds (np.ndarray): Predicted labels for the dataset.\n",
    "#     - all_labels (np.ndarray): True labels for the dataset.\n",
    "#     \"\"\"\n",
    "#     encoder.eval()\n",
    "#     classifier.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for x, y in data_loader:\n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "            \n",
    "#             # Generate embeddings with the encoder\n",
    "#             z = encoder(x)\n",
    "            \n",
    "#             # Classifier outputs\n",
    "#             outputs = classifier(z)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             # Collect predictions and labels\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(y.cpu().numpy())\n",
    "    \n",
    "#     return np.array(all_preds), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_predictions import get_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sup_training_classifier import train_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.4623, Val Loss: 1.5836, Val Acc: 0.2976\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [2/10], Train Loss: 1.0011, Val Loss: 1.5375, Val Acc: 0.6548\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [3/10], Train Loss: 0.8928, Val Loss: 1.4806, Val Acc: 0.6845\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [4/10], Train Loss: 0.8572, Val Loss: 1.4162, Val Acc: 0.6786\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [5/10], Train Loss: 0.8035, Val Loss: 1.3507, Val Acc: 0.7024\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [6/10], Train Loss: 0.8038, Val Loss: 1.2815, Val Acc: 0.7024\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [7/10], Train Loss: 0.7611, Val Loss: 1.2128, Val Acc: 0.7083\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [8/10], Train Loss: 0.7659, Val Loss: 1.1394, Val Acc: 0.7083\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [9/10], Train Loss: 0.7156, Val Loss: 1.0674, Val Acc: 0.7024\n",
      "Saved best model to best_classifier.pth\n",
      "Epoch [10/10], Train Loss: 0.7391, Val Loss: 0.9931, Val Acc: 0.6964\n",
      "Saved best model to best_classifier.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9930530488491058"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SleepStageClassifier(input_dim=128, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# Freeze the encoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "train_classifier(model, classifier, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda', save_path='best_classifier.pth') #num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.7219\n",
      "Macro F1 Score: 0.5888\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.86      0.50      0.63        38\n",
      "     Class 1       0.00      0.00      0.00        12\n",
      "     Class 2       0.83      0.78      0.80        50\n",
      "     Class 3       0.91      0.89      0.90        44\n",
      "     Class 4       0.44      1.00      0.61        25\n",
      "\n",
      "    accuracy                           0.72       169\n",
      "   macro avg       0.61      0.63      0.59       169\n",
      "weighted avg       0.74      0.72      0.70       169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shasw\\AppData\\Local\\Temp\\ipykernel_14468\\799435996.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load('best_classifier.pth'))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "\n",
    "classifier.load_state_dict(torch.load('best_classifier.pth'))\n",
    "predictions, true_labels = get_predictions(model, classifier, test_loader)\n",
    "\n",
    "# Overall Accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1 Score per Class\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(\n",
    "    true_labels, predictions, labels=range(num_classes)\n",
    ")\n",
    "\n",
    "# Macro F1 Score\n",
    "macro_f1 = np.mean(f1_score)\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, labels=range(num_classes), target_names=[f\"Class {i}\" for i in range(num_classes)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12,16,11, 10,7,6,4\n",
    "\n",
    "go to each module and suggest improvements and doc strings and ask it to explain everything in the you can also improve the logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:54:29,916 - __main__ - INFO - Starting the EEG Project\n",
      "2024-11-24 19:54:30,479 - __main__ - INFO - Using device: cuda\n",
      "2024-11-24 19:54:30,482 - __main__ - INFO - EEG data loaded. Number of classes: 5\n",
      "2024-11-24 19:54:30,498 - __main__ - INFO - Contrastive dataset created and loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 152 npz files in ./dset/Sleep-EDF-2018/npz/Fpz-Cz.\n",
      "Processed 1/1 files.\n",
      "#EEG Epochs for Label 0 : 188\n",
      "#EEG Epochs for Label 1 : 58\n",
      "#EEG Epochs for Label 2 : 250\n",
      "#EEG Epochs for Label 3 : 220\n",
      "#EEG Epochs for Label 4 : 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:54:31,249 - __main__ - INFO - Model and optimizer created.\n",
      "2024-11-24 19:54:32,883 - __main__ - INFO - Contrastive pretraining completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Training - Epoch [1/1], Loss: 5.3580\n",
      "Starting Latent Space Evaluation...\n",
      "Embeddings extracted and scaled.\n",
      "Applying t-SNE...\n",
      "t-SNE completed.\n",
      "Applying UMAP...\n",
      "UMAP completed.\n",
      "Applying PCA...\n",
      "PCA completed.\n",
      "Evaluating metrics...\n",
      "Metrics for t-SNE saved to latent_space_metrics\\t-sne_metrics_1.csv.\n",
      "Metrics for UMAP saved to latent_space_metrics\\umap_metrics_1.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for PCA saved to latent_space_metrics\\pca_metrics_1.csv.\n",
      "Evaluation metrics computed and saved.\n",
      "Visualizing embeddings...\n",
      "Visualization for t-SNE saved to latent_space_viz\\t-sne_embeddings_1.png.\n",
      "Visualization for UMAP saved to latent_space_viz\\umap_embeddings_1.png.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:54:42,150 - root - INFO - Latent space evaluation completed.\n",
      "2024-11-24 19:54:42,166 - __main__ - INFO - Supervised dataset created and loaded.\n",
      "2024-11-24 19:54:42,179 - __main__ - INFO - Classifier created.\n",
      "2024-11-24 19:54:42,179 - __main__ - INFO - Encoder frozen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization for PCA saved to latent_space_viz\\pca_embeddings_1.png.\n",
      "Embeddings visualized and saved.\n",
      "Latent Space Evaluation Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 19:54:42,403 - __main__ - INFO - Classifier training completed.\n",
      "C:\\Users\\shasw\\AppData\\Local\\Temp\\ipykernel_22796\\2584624616.py:168: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(config[\"sup_training_params\"][\"best_model_pth\"]))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\shasw\\.conda\\envs\\sleepyco\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "2024-11-24 19:54:42,462 - __main__ - INFO - Classification results saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.5006, Val Loss: 1.5879, Val Acc: 0.5298\n",
      "Saved best model to best_classifier.pth\n",
      "Overall Accuracy: 0.5385\n",
      "Macro F1 Score: 0.2867\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score     support\n",
      "Class 0        0.000000  0.000000  0.000000   38.000000\n",
      "Class 1        0.000000  0.000000  0.000000   12.000000\n",
      "Class 2        0.442478  1.000000  0.613497   50.000000\n",
      "Class 3        0.732143  0.931818  0.820000   44.000000\n",
      "Class 4        0.000000  0.000000  0.000000   25.000000\n",
      "accuracy       0.538462  0.538462  0.538462    0.538462\n",
      "macro avg      0.234924  0.386364  0.286699  169.000000\n",
      "weighted avg   0.321528  0.538462  0.394999  169.000000\n",
      "Results saved to results\\overall_1.csv and results\\perclass_1.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "from datasets import ContrastiveEEGDataset, SupervisedEEGDataset\n",
    "from models import SimpleSleepNet, SleepStageClassifier\n",
    "from training import train_contrastive, train_classifier\n",
    "from evaluation import LatentSpaceEvaluator, get_predictions, save_classification_results\n",
    "from augmentations import *\n",
    "from utils import load_eeg_data\n",
    "from logging_config import setup_logging\n",
    "\n",
    "\n",
    "config_path = 'config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "setup_logging(log_level=logging.INFO, log_file=f'logs/experiment_{config[\"experiment_num\"]}.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Starting the EEG Project\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "BATCH_SIZE = config[\"pretraining_params\"][\"batch_size\"]\n",
    "FS = 100\n",
    "EPOCH_LENGTH = 30\n",
    "SAMPLES_PER_EPOCH = FS * EPOCH_LENGTH\n",
    "NUM_CLASSES = 5\n",
    "EPOCHS_PER_CLASS = 10\n",
    "\n",
    "\n",
    "\n",
    "eeg_data = load_eeg_data(dset_path=config['dataset']['dset_path'], num_files=config['dataset']['max_files'])\n",
    "logger.info(f\"EEG data loaded. Number of classes: {len(eeg_data)}\")\n",
    "\n",
    "\n",
    "AUGMENTATION_CLASSES = {\n",
    "    \"RandomAmplitudeScale\": RandomAmplitudeScale,\n",
    "    \"RandomDCShift\": RandomDCShift,\n",
    "    \"RandomTimeShift\": RandomTimeShift,\n",
    "    \"RandomZeroMasking\": RandomZeroMasking,\n",
    "    \"RandomAdditiveGaussianNoise\": RandomAdditiveGaussianNoise,\n",
    "    \"RandomBandStopFilter\": RandomBandStopFilter,\n",
    "    \"TimeWarping\": TimeWarping,\n",
    "    \"TimeReverse\": TimeReverse,\n",
    "    \"Permutation\": Permutation,\n",
    "    \"CutoutResize\": CutoutResize,\n",
    "    \"TailoredMixup\": TailoredMixup,\n",
    "    \"AverageFilter\": AverageFilter,\n",
    "    \"SignFlip\": SignFlip\n",
    "}\n",
    "\n",
    "augmentations = []\n",
    "for aug_name, aug_params in config.get(\"augmentations\", {}).items():\n",
    "    if aug_name in AUGMENTATION_CLASSES:\n",
    "        augmentation_class = AUGMENTATION_CLASSES[aug_name]\n",
    "        augmentations.append(augmentation_class(**aug_params))\n",
    "    else:\n",
    "        logger.warning(f\"Warning: Augmentation '{aug_name}' not recognized. Skipping.\")\n",
    "\n",
    "contrastive_dataset = ContrastiveEEGDataset(eeg_data, augmentations=augmentations)\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=BATCH_SIZE, shuffle=True)  #batchsize = 64\n",
    "logger.info(\"Contrastive dataset created and loaded.\")\n",
    "\n",
    "model = SimpleSleepNet(latent_dim=config[\"pretraining_params\"][\"latent_dim\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"pretraining_params\"][\"learning_rate\"])\n",
    "logger.info(\"Model and optimizer created.\")\n",
    "\n",
    "train_contrastive(model, contrastive_loader, optimizer, num_epochs=config[\"pretraining_params\"][\"num_epochs\"]) # num_epochs = 500\n",
    "logger.info(\"Contrastive pretraining completed.\")\n",
    "\n",
    "visualization_dataset = ContrastiveEEGDataset(eeg_data= eeg_data, augmentations=[], return_labels=True)\n",
    "visualization_loader = DataLoader(visualization_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "evaluator = LatentSpaceEvaluator(\n",
    "    model=model,\n",
    "    dataloader=visualization_loader,\n",
    "    device=device,\n",
    "    umap_enabled=config[\"latent_space_params\"][\"umap_enabled\"],\n",
    "    pca_enabled=config[\"latent_space_params\"][\"pca_enabled\"],\n",
    "    tsne_enabled=config[\"latent_space_params\"][\"tsne_enabled\"],\n",
    "    visualize=config[\"latent_space_params\"][\"visualize\"],\n",
    "    compute_metrics=config[\"latent_space_params\"][\"compute_metrics\"],\n",
    "    n_clusters=config[\"latent_space_params\"][\"n_clusters\"],\n",
    "    output_image_dir=config[\"latent_space_params\"][\"output_image_dir\"],   \n",
    "    output_metrics_dir=config[\"latent_space_params\"][\"output_metrics_dir\"], \n",
    "    augmentation_strategy=config[\"experiment_num\"]\n",
    ")\n",
    "\n",
    "evaluator.run()\n",
    "logging.info(\"Latent space evaluation completed.\")\n",
    "\n",
    "\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for label, signals in eeg_data.items():\n",
    "    all_data.extend(signals)\n",
    "    all_labels.extend([label] * len(signals))\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# First split into train+val and test sets (80% train+val, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    all_data, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Then split train_val into train and val sets (75% train, 25% val of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42\n",
    ")\n",
    "# Now we have 60% train, 20% val, 20% test\n",
    "\n",
    "# Create dictionaries to store data by label\n",
    "num_classes = len(np.unique(all_labels))\n",
    "train_data = {label: [] for label in range(num_classes)}\n",
    "val_data = {label: [] for label in range(num_classes)}\n",
    "test_data = {label: [] for label in range(num_classes)}\n",
    "\n",
    "# Fill the dictionaries\n",
    "for x, y in zip(X_train, y_train):\n",
    "    train_data[y].append(x)\n",
    "for x, y in zip(X_val, y_val):\n",
    "    val_data[y].append(x)\n",
    "for x, y in zip(X_test, y_test):\n",
    "    test_data[y].append(x)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for label in train_data:\n",
    "    train_data[label] = np.array(train_data[label])\n",
    "    val_data[label] = np.array(val_data[label])\n",
    "    test_data[label] = np.array(test_data[label])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SupervisedEEGDataset(train_data)\n",
    "val_dataset = SupervisedEEGDataset(val_data)\n",
    "test_dataset = SupervisedEEGDataset(test_data)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "logger.info(\"Supervised dataset created and loaded.\")\n",
    "\n",
    "### --- Train Classifier ---###\n",
    "classifier = SleepStageClassifier(input_dim=config[\"pretraining_params\"][\"latent_dim\"], num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=config[\"sup_training_params\"][\"learning_rate\"])\n",
    "logger.info(\"Classifier created.\")\n",
    "\n",
    "# Freeze the encoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "logger.info(\"Encoder frozen.\")\n",
    "\n",
    "train_classifier(model, classifier, train_loader, val_loader, criterion, optimizer, num_epochs=config[\"sup_training_params\"][\"num_epochs\"], device=device, save_path=config[\"sup_training_params\"][\"best_model_pth\"]) #num_epochs = 100\n",
    "logger.info(\"Classifier training completed.\")\n",
    "\n",
    "classifier.load_state_dict(torch.load(config[\"sup_training_params\"][\"best_model_pth\"]))\n",
    "predictions, true_labels = get_predictions(model, classifier, test_loader, device=device)\n",
    "save_classification_results(results_folder = config[\"results_folder\"], predictions = predictions, true_labels=true_labels, num_classes=num_classes, experiment_num = config[\"experiment_num\"])\n",
    "logger.info(\"Classification results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# def save_classification_results(results_folder, predictions, true_labels, num_classes, experiment_num):\n",
    "#     \"\"\"\n",
    "#     Saves classification results to Excel files.\n",
    "\n",
    "#     Parameters:\n",
    "#     - results_folder (str): Path to the folder where results will be saved.\n",
    "#     - predictions (np.ndarray): Predicted labels.\n",
    "#     - true_labels (np.ndarray): True labels.\n",
    "#     - num_classes (int): Number of classes.\n",
    "#     - experiment_num (int): Experiment number for file naming.\n",
    "\n",
    "#     Outputs:\n",
    "#     - Saves overall metrics and per-class metrics to separate Excel files in the specified folder.\n",
    "#     \"\"\"\n",
    "#     # Create the folder if it doesn't exist\n",
    "#     os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(true_labels, predictions)\n",
    "#     precision, recall, f1_score, support = precision_recall_fscore_support(\n",
    "#         true_labels, predictions, labels=range(num_classes)\n",
    "#     )\n",
    "#     macro_f1 = np.mean(f1_score)\n",
    "\n",
    "#     # Save overall metrics\n",
    "#     overall_metrics = {\n",
    "#         \"Metric\": [\"Overall Accuracy\", \"Macro F1\"],\n",
    "#         \"Value\": [accuracy, macro_f1]\n",
    "#     }\n",
    "#     overall_metrics_df = pd.DataFrame(overall_metrics)\n",
    "#     overall_metrics_path = os.path.join(results_folder, f'overall_{experiment_num}.csv')\n",
    "#     overall_metrics_df.to_csv(overall_metrics_path, index=False)\n",
    "\n",
    "#     print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "#     print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "#     # Save per-class metrics\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     class_report = classification_report(\n",
    "#         true_labels,\n",
    "#         predictions,\n",
    "#         labels=range(num_classes),\n",
    "#         target_names=[f\"Class {i}\" for i in range(num_classes)],\n",
    "#         output_dict=True\n",
    "#     )\n",
    "#     print(pd.DataFrame(class_report).transpose())\n",
    "\n",
    "#     class_metrics_df = pd.DataFrame(class_report).transpose()\n",
    "#     class_metrics_path = os.path.join(results_folder, f'perclass_{experiment_num}.csv')\n",
    "#     class_metrics_df.to_csv(class_metrics_path, index=True)\n",
    "\n",
    "#     print(f\"Results saved to {overall_metrics_path} and {class_metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "{\n",
      "    \"name\": \"Sleep-EDF-2018\",\n",
      "    \"eeg_channel\": \"Fpz-Cz\",\n",
      "    \"num_splits\": 10,\n",
      "    \"seq_len\": 1,\n",
      "    \"target_idx\": 0,\n",
      "    \"root_dir\": \"./\"\n",
      "}\n",
      "Found 152 npz files in ./dset\\Sleep-EDF-2018\\npz\\Fpz-Cz.\n",
      "Loaded data from ./dset\\Sleep-EDF-2018\\npz\\Fpz-Cz\\SC4001E0.npz:\n",
      "EEG data shape (epochs, samples): (841, 3000)\n",
      "Labels shape: (841,)\n",
      "Percentage of each class :\n",
      "[[ 0.         22.35434007]\n",
      " [ 1.          6.89655172]\n",
      " [ 2.         29.72651605]\n",
      " [ 3.         26.15933413]\n",
      " [ 4.         14.86325803]]\n",
      "#EEG Epochs for Label 0 : 188\n",
      "#EEG Epochs for Label 1 : 58\n",
      "#EEG Epochs for Label 2 : 250\n",
      "#EEG Epochs for Label 3 : 220\n",
      "#EEG Epochs for Label 4 : 125\n",
      "Using device: cuda\n",
      "Contrastive Training - Epoch [1/10], Loss: 5.3529\n",
      "Contrastive Training - Epoch [2/10], Loss: 5.3276\n",
      "Contrastive Training - Epoch [3/10], Loss: 5.3214\n",
      "Contrastive Training - Epoch [4/10], Loss: 5.2779\n",
      "Contrastive Training - Epoch [5/10], Loss: 5.2712\n",
      "Contrastive Training - Epoch [6/10], Loss: 5.2412\n",
      "Contrastive Training - Epoch [7/10], Loss: 5.1966\n",
      "Contrastive Training - Epoch [8/10], Loss: 5.1695\n",
      "Contrastive Training - Epoch [9/10], Loss: 5.1318\n",
      "Contrastive Training - Epoch [10/10], Loss: 5.1130\n",
      "Starting Latent Space Evaluation...\n",
      "Embeddings extracted and scaled.\n",
      "Applying t-SNE...\n",
      "t-SNE completed.\n",
      "Applying UMAP...\n",
      "UMAP completed.\n",
      "Applying PCA...\n",
      "PCA completed.\n",
      "Evaluating metrics...\n",
      "Metrics for t-SNE saved to latent_space_metrics\\t-sne_metrics_1.csv.\n",
      "Metrics for UMAP saved to latent_space_metrics\\umap_metrics_1.csv.\n",
      "Metrics for PCA saved to latent_space_metrics\\pca_metrics_1.csv.\n",
      "Evaluation metrics computed and saved.\n",
      "Visualizing embeddings...\n",
      "Visualization for t-SNE saved to latent_space_viz\\t-sne_embeddings_1.png.\n",
      "Visualization for UMAP saved to latent_space_viz\\umap_embeddings_1.png.\n",
      "Visualization for PCA saved to latent_space_viz\\pca_embeddings_1.png.\n",
      "Embeddings visualized and saved.\n",
      "Latent Space Evaluation Completed.\n",
      "Epoch [1/100], Train Loss: 1.3508, Val Loss: 1.5738, Val Acc: 0.3929\n",
      "Epoch [2/100], Train Loss: 1.0151, Val Loss: 1.5399, Val Acc: 0.5179\n",
      "Epoch [3/100], Train Loss: 0.8708, Val Loss: 1.4966, Val Acc: 0.4762\n",
      "Epoch [4/100], Train Loss: 0.8127, Val Loss: 1.4438, Val Acc: 0.5238\n",
      "Epoch [5/100], Train Loss: 0.7819, Val Loss: 1.3810, Val Acc: 0.5833\n",
      "Epoch [6/100], Train Loss: 0.7549, Val Loss: 1.3117, Val Acc: 0.6369\n",
      "Epoch [7/100], Train Loss: 0.7229, Val Loss: 1.2354, Val Acc: 0.6488\n",
      "Epoch [8/100], Train Loss: 0.7133, Val Loss: 1.1534, Val Acc: 0.6845\n",
      "Epoch [9/100], Train Loss: 0.7130, Val Loss: 1.0612, Val Acc: 0.6964\n",
      "Epoch [10/100], Train Loss: 0.7054, Val Loss: 0.9778, Val Acc: 0.7083\n",
      "Epoch [11/100], Train Loss: 0.6915, Val Loss: 0.9043, Val Acc: 0.7024\n",
      "Epoch [12/100], Train Loss: 0.6841, Val Loss: 0.8466, Val Acc: 0.7083\n",
      "Epoch [13/100], Train Loss: 0.6738, Val Loss: 0.8043, Val Acc: 0.7024\n",
      "Epoch [14/100], Train Loss: 0.6899, Val Loss: 0.7753, Val Acc: 0.7083\n",
      "Epoch [15/100], Train Loss: 0.6592, Val Loss: 0.7470, Val Acc: 0.7143\n",
      "Epoch [16/100], Train Loss: 0.6724, Val Loss: 0.7268, Val Acc: 0.7143\n",
      "Epoch [17/100], Train Loss: 0.6245, Val Loss: 0.7077, Val Acc: 0.7262\n",
      "Epoch [18/100], Train Loss: 0.6412, Val Loss: 0.6913, Val Acc: 0.7262\n",
      "Epoch [19/100], Train Loss: 0.6149, Val Loss: 0.6904, Val Acc: 0.7321\n",
      "Epoch [20/100], Train Loss: 0.6274, Val Loss: 0.7052, Val Acc: 0.7202\n",
      "Epoch [21/100], Train Loss: 0.6278, Val Loss: 0.7100, Val Acc: 0.7321\n",
      "Epoch [22/100], Train Loss: 0.6052, Val Loss: 0.7024, Val Acc: 0.7440\n",
      "Epoch [23/100], Train Loss: 0.6084, Val Loss: 0.6905, Val Acc: 0.7381\n",
      "Epoch [24/100], Train Loss: 0.6056, Val Loss: 0.6800, Val Acc: 0.7381\n",
      "Epoch [25/100], Train Loss: 0.5965, Val Loss: 0.6836, Val Acc: 0.7440\n",
      "Epoch [26/100], Train Loss: 0.5645, Val Loss: 0.6860, Val Acc: 0.7440\n",
      "Epoch [27/100], Train Loss: 0.5592, Val Loss: 0.7003, Val Acc: 0.7500\n",
      "Epoch [28/100], Train Loss: 0.5833, Val Loss: 0.7111, Val Acc: 0.7381\n",
      "Epoch [29/100], Train Loss: 0.5580, Val Loss: 0.7155, Val Acc: 0.7381\n",
      "Epoch [30/100], Train Loss: 0.5601, Val Loss: 0.6912, Val Acc: 0.7381\n",
      "Epoch [31/100], Train Loss: 0.5623, Val Loss: 0.6749, Val Acc: 0.7440\n",
      "Epoch [32/100], Train Loss: 0.5680, Val Loss: 0.6924, Val Acc: 0.7381\n",
      "Epoch [33/100], Train Loss: 0.5596, Val Loss: 0.7182, Val Acc: 0.7262\n",
      "Epoch [34/100], Train Loss: 0.5723, Val Loss: 0.7129, Val Acc: 0.7321\n",
      "Epoch [35/100], Train Loss: 0.5479, Val Loss: 0.6833, Val Acc: 0.7440\n",
      "Epoch [36/100], Train Loss: 0.5433, Val Loss: 0.6970, Val Acc: 0.7381\n",
      "Epoch [37/100], Train Loss: 0.5390, Val Loss: 0.7063, Val Acc: 0.7440\n",
      "Epoch [38/100], Train Loss: 0.5395, Val Loss: 0.7309, Val Acc: 0.7500\n",
      "Epoch [39/100], Train Loss: 0.4957, Val Loss: 0.7147, Val Acc: 0.7321\n",
      "Epoch [40/100], Train Loss: 0.5322, Val Loss: 0.7098, Val Acc: 0.7381\n",
      "Epoch [41/100], Train Loss: 0.5400, Val Loss: 0.7145, Val Acc: 0.7500\n",
      "Epoch [42/100], Train Loss: 0.5162, Val Loss: 0.7090, Val Acc: 0.7500\n",
      "Epoch [43/100], Train Loss: 0.4878, Val Loss: 0.7296, Val Acc: 0.7321\n",
      "Epoch [44/100], Train Loss: 0.5292, Val Loss: 0.7216, Val Acc: 0.7381\n",
      "Epoch [45/100], Train Loss: 0.4865, Val Loss: 0.7043, Val Acc: 0.7440\n",
      "Epoch [46/100], Train Loss: 0.4981, Val Loss: 0.7015, Val Acc: 0.7381\n",
      "Epoch [47/100], Train Loss: 0.4856, Val Loss: 0.7202, Val Acc: 0.7321\n",
      "Epoch [48/100], Train Loss: 0.4727, Val Loss: 0.7343, Val Acc: 0.7440\n",
      "Epoch [49/100], Train Loss: 0.5160, Val Loss: 0.7369, Val Acc: 0.7440\n",
      "Epoch [50/100], Train Loss: 0.4826, Val Loss: 0.7307, Val Acc: 0.7440\n",
      "Epoch [51/100], Train Loss: 0.4787, Val Loss: 0.7437, Val Acc: 0.7440\n",
      "Epoch [52/100], Train Loss: 0.4534, Val Loss: 0.7250, Val Acc: 0.7262\n",
      "Epoch [53/100], Train Loss: 0.4878, Val Loss: 0.7264, Val Acc: 0.7262\n",
      "Epoch [54/100], Train Loss: 0.4873, Val Loss: 0.7525, Val Acc: 0.7381\n",
      "Epoch [55/100], Train Loss: 0.4497, Val Loss: 0.7594, Val Acc: 0.7500\n",
      "Epoch [56/100], Train Loss: 0.4671, Val Loss: 0.7647, Val Acc: 0.7500\n",
      "Epoch [57/100], Train Loss: 0.4783, Val Loss: 0.7433, Val Acc: 0.7440\n",
      "Epoch [58/100], Train Loss: 0.4720, Val Loss: 0.7209, Val Acc: 0.7321\n",
      "Epoch [59/100], Train Loss: 0.4519, Val Loss: 0.7154, Val Acc: 0.7440\n",
      "Epoch [60/100], Train Loss: 0.4477, Val Loss: 0.7102, Val Acc: 0.7440\n",
      "Epoch [61/100], Train Loss: 0.4389, Val Loss: 0.7369, Val Acc: 0.7619\n",
      "Epoch [62/100], Train Loss: 0.4478, Val Loss: 0.7276, Val Acc: 0.7500\n",
      "Epoch [63/100], Train Loss: 0.4368, Val Loss: 0.7402, Val Acc: 0.7440\n",
      "Epoch [64/100], Train Loss: 0.4495, Val Loss: 0.7459, Val Acc: 0.7440\n",
      "Epoch [65/100], Train Loss: 0.4314, Val Loss: 0.7304, Val Acc: 0.7321\n",
      "Epoch [66/100], Train Loss: 0.4410, Val Loss: 0.7309, Val Acc: 0.7321\n",
      "Epoch [67/100], Train Loss: 0.4554, Val Loss: 0.7806, Val Acc: 0.7321\n",
      "Epoch [68/100], Train Loss: 0.4317, Val Loss: 0.7841, Val Acc: 0.7381\n",
      "Epoch [69/100], Train Loss: 0.4485, Val Loss: 0.7569, Val Acc: 0.7381\n",
      "Epoch [70/100], Train Loss: 0.4257, Val Loss: 0.7430, Val Acc: 0.7560\n",
      "Epoch [71/100], Train Loss: 0.4462, Val Loss: 0.7814, Val Acc: 0.7679\n",
      "Epoch [72/100], Train Loss: 0.4054, Val Loss: 0.7691, Val Acc: 0.7381\n",
      "Epoch [73/100], Train Loss: 0.4373, Val Loss: 0.7587, Val Acc: 0.7500\n",
      "Epoch [74/100], Train Loss: 0.4233, Val Loss: 0.7459, Val Acc: 0.7500\n",
      "Epoch [75/100], Train Loss: 0.3994, Val Loss: 0.7742, Val Acc: 0.7619\n",
      "Epoch [76/100], Train Loss: 0.4242, Val Loss: 0.7678, Val Acc: 0.7381\n",
      "Epoch [77/100], Train Loss: 0.3930, Val Loss: 0.7610, Val Acc: 0.7500\n",
      "Epoch [78/100], Train Loss: 0.4229, Val Loss: 0.7532, Val Acc: 0.7560\n",
      "Epoch [79/100], Train Loss: 0.3948, Val Loss: 0.7743, Val Acc: 0.7560\n",
      "Epoch [80/100], Train Loss: 0.3793, Val Loss: 0.7759, Val Acc: 0.7679\n",
      "Epoch [81/100], Train Loss: 0.4282, Val Loss: 0.7964, Val Acc: 0.7738\n",
      "Epoch [82/100], Train Loss: 0.3802, Val Loss: 0.7836, Val Acc: 0.7679\n",
      "Epoch [83/100], Train Loss: 0.4007, Val Loss: 0.7666, Val Acc: 0.7500\n",
      "Epoch [84/100], Train Loss: 0.3988, Val Loss: 0.7974, Val Acc: 0.7560\n",
      "Epoch [85/100], Train Loss: 0.4091, Val Loss: 0.7880, Val Acc: 0.7679\n",
      "Epoch [86/100], Train Loss: 0.4105, Val Loss: 0.7900, Val Acc: 0.7619\n",
      "Epoch [87/100], Train Loss: 0.3841, Val Loss: 0.7980, Val Acc: 0.7381\n",
      "Epoch [88/100], Train Loss: 0.3892, Val Loss: 0.8121, Val Acc: 0.7381\n",
      "Epoch [89/100], Train Loss: 0.3847, Val Loss: 0.8016, Val Acc: 0.7500\n",
      "Epoch [90/100], Train Loss: 0.3706, Val Loss: 0.8032, Val Acc: 0.7619\n",
      "Epoch [91/100], Train Loss: 0.3846, Val Loss: 0.8079, Val Acc: 0.7619\n",
      "Epoch [92/100], Train Loss: 0.3814, Val Loss: 0.8256, Val Acc: 0.7560\n",
      "Epoch [93/100], Train Loss: 0.3891, Val Loss: 0.8121, Val Acc: 0.7619\n",
      "Epoch [94/100], Train Loss: 0.3354, Val Loss: 0.8373, Val Acc: 0.7440\n",
      "Epoch [95/100], Train Loss: 0.3575, Val Loss: 0.8180, Val Acc: 0.7500\n",
      "Epoch [96/100], Train Loss: 0.3700, Val Loss: 0.8001, Val Acc: 0.7381\n",
      "Epoch [97/100], Train Loss: 0.3734, Val Loss: 0.8315, Val Acc: 0.7500\n",
      "Epoch [98/100], Train Loss: 0.3516, Val Loss: 0.8431, Val Acc: 0.7440\n",
      "Epoch [99/100], Train Loss: 0.3738, Val Loss: 0.8173, Val Acc: 0.7440\n",
      "Epoch [100/100], Train Loss: 0.3584, Val Loss: 0.8377, Val Acc: 0.7560\n",
      "Overall Accuracy: 0.7929\n",
      "Macro F1 Score: 0.6839\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.74      0.85        38\n",
      "     Class 1       0.50      0.08      0.14        12\n",
      "     Class 2       0.74      0.86      0.80        50\n",
      "     Class 3       0.95      0.86      0.90        44\n",
      "     Class 4       0.59      0.96      0.73        25\n",
      "\n",
      "    accuracy                           0.79       169\n",
      "   macro avg       0.76      0.70      0.68       169\n",
      "weighted avg       0.81      0.79      0.78       169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shasw\\AppData\\Local\\Temp\\ipykernel_12396\\2269624297.py:177: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load('best_classifier.pth'))\n"
     ]
    }
   ],
   "source": [
    "### --- Load EEG Data --- ###\n",
    "config_path = 'sample_config_2018.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "dset_cfg = config['dataset']\n",
    "root_dir = dset_cfg['root_dir']\n",
    "dset_name = dset_cfg['name']\n",
    "eeg_channel = dset_cfg['eeg_channel']\n",
    "num_splits = dset_cfg['num_splits']\n",
    "seq_len = dset_cfg['seq_len']\n",
    "target_idx = dset_cfg['target_idx']\n",
    "\n",
    "print(\"Configuration Loaded:\")\n",
    "print(json.dumps(dset_cfg, indent=4))\n",
    "\n",
    "\n",
    "dataset_path = os.path.join(root_dir, 'dset', dset_name, 'npz', eeg_channel)\n",
    "npz_files = sorted(glob.glob(os.path.join(dataset_path, '*.npz')))\n",
    "\n",
    "print(f\"Found {len(npz_files)} npz files in {dataset_path}.\")\n",
    "\n",
    "sample_file = npz_files[0]\n",
    "data = np.load(sample_file)\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "print(f\"Loaded data from {sample_file}:\")\n",
    "print(\"EEG data shape (epochs, samples):\", x.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# % of each class:\n",
    "print(\"Percentage of each class :\")\n",
    "print(np.asarray((unique, counts*100/np.sum(counts))).T)\n",
    "\n",
    "fs = 100\n",
    "epoch_length = 30\n",
    "samples_per_epoch = fs * epoch_length\n",
    "num_classes = 5\n",
    "epochs_per_class = 10\n",
    "\n",
    "eeg_data = {i: [] for i in range(5)}\n",
    "\n",
    "for file_idx, file in enumerate(npz_files, start=1):\n",
    "    with np.load(file) as data:\n",
    "        x = data['x']  # EEG epochs\n",
    "        y = data['y']  # Corresponding labels\n",
    "\n",
    "        for label in range(5):\n",
    "            epochs = x[y == label]\n",
    "            eeg_data[label].extend(epochs)\n",
    "    \n",
    "    if file_idx % 10 == 0 or file_idx == len(npz_files):\n",
    "        print(f\"Processed {file_idx}/{len(npz_files)} files.\")\n",
    "    break                                                                        # Remove this break - this is to only import 1 file so that our loop runs quickly and we can see our code infra is working.\n",
    "\n",
    "for label in eeg_data:\n",
    "    eeg_data[label] = np.array(eeg_data[label])\n",
    "    print(f\"#EEG Epochs for Label {label} : {len(eeg_data[label])}\")\n",
    "\n",
    "\n",
    "\n",
    "### --- Pre-Train Encdoer with CRL --- ###\n",
    "augmentations = [RandomAmplitudeScale(p=1.0),TailoredMixup(p=1.0),RandomAdditiveGaussianNoise(p=1.0)]\n",
    "\n",
    "contrastive_dataset = ContrastiveEEGDataset(eeg_data, augmentations=augmentations)\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=128, shuffle=True)  #batchsize = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SimpleSleepNet(latent_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_contrastive(model, contrastive_loader, optimizer, num_epochs=10) # num_epochs = 500\n",
    "\n",
    "\n",
    "### --- Assess Latent Space --- ###\n",
    "\n",
    "# Initialize the dataset with return_labels=True\n",
    "visualization_dataset = ContrastiveEEGDataset(\n",
    "    eeg_data= eeg_data,\n",
    "    augmentations=[],\n",
    "    return_labels=True\n",
    ")\n",
    "\n",
    "visualization_loader = DataLoader(visualization_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "evaluator = LatentSpaceEvaluator(\n",
    "    model=model,\n",
    "    dataloader=visualization_loader,\n",
    "    device='cuda',                   # or 'cpu'\n",
    "    umap_enabled=True,\n",
    "    pca_enabled=True,\n",
    "    tsne_enabled=True,\n",
    "    visualize=True,\n",
    "    compute_metrics=True,\n",
    "    n_clusters=5,                   # Number of expected clusters/classes\n",
    "    output_image_dir='latent_space_viz',    # Specify your desired image output directory\n",
    "    output_metrics_dir='latent_space_metrics', # Specify your desired metrics output directory\n",
    "    augmentation_strategy='1' # Replace with your augmentation strategy identifier\n",
    ")\n",
    "\n",
    "evaluator.run()\n",
    "\n",
    "### --- Train Test Val Split --- ###\n",
    "\n",
    "# Combine all data and labels\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for label, signals in eeg_data.items():\n",
    "    all_data.extend(signals)\n",
    "    all_labels.extend([label] * len(signals))\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# First split into train+val and test sets (80% train+val, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    all_data, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
    ")\n",
    "\n",
    "# Then split train_val into train and val sets (75% train, 25% val of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42\n",
    ")\n",
    "# Now we have 60% train, 20% val, 20% test\n",
    "\n",
    "# Create dictionaries to store data by label\n",
    "num_classes = len(np.unique(all_labels))\n",
    "train_data = {label: [] for label in range(num_classes)}\n",
    "val_data = {label: [] for label in range(num_classes)}\n",
    "test_data = {label: [] for label in range(num_classes)}\n",
    "\n",
    "# Fill the dictionaries\n",
    "for x, y in zip(X_train, y_train):\n",
    "    train_data[y].append(x)\n",
    "for x, y in zip(X_val, y_val):\n",
    "    val_data[y].append(x)\n",
    "for x, y in zip(X_test, y_test):\n",
    "    test_data[y].append(x)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for label in train_data:\n",
    "    train_data[label] = np.array(train_data[label])\n",
    "    val_data[label] = np.array(val_data[label])\n",
    "    test_data[label] = np.array(test_data[label])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SupervisedEEGDataset(train_data)\n",
    "val_dataset = SupervisedEEGDataset(val_data)\n",
    "test_dataset = SupervisedEEGDataset(test_data)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128   #64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "### --- Train Classifier ---###\n",
    "classifier = SleepStageClassifier(input_dim=128, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# Freeze the encoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "train_classifier(model, classifier, train_loader, val_loader, criterion, optimizer, num_epochs=100)\n",
    "\n",
    "\n",
    "### ---- Measure Performance of Netowork --- ###\n",
    "\n",
    "classifier.load_state_dict(torch.load('best_classifier.pth'))\n",
    "predictions, true_labels = get_predictions(model, classifier, test_loader)\n",
    "\n",
    "# Overall Accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1 Score per Class\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(\n",
    "    true_labels, predictions, labels=range(num_classes)\n",
    ")\n",
    "\n",
    "# Macro F1 Score\n",
    "macro_f1 = np.mean(f1_score)\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, labels=range(num_classes), target_names=[f\"Class {i}\" for i in range(num_classes)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleepyco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
